{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d392ce-c570-4b3b-b116-15719a56b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. \n",
    "\n",
    "# Projection in PCA:\n",
    "\n",
    "# Projection is the transformation of data onto a lower-dimensional subspace.\n",
    "# PCA identifies principal components representing directions of maximum variance.\n",
    "# Data is projected onto these principal components to achieve dimenisonality reduction.\n",
    "# Purpose: Captures the most critical information while discarding less significant dimensions.\n",
    "# Mathematical Representation: Projection involves a linear combination of original features weighted by principal component coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495f049f-1f81-4229-b7e7-33e496380200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "# Optimization Problem in PCA:\n",
    "# Goal is to maximize variance captured by linear combinations of original features.\n",
    "# Covariance Matrix:\n",
    "# Calculation: PCA involves computing the covariance matrix of the data.\n",
    "# Eigenvalue Decomposition: Diagonalization of the covariance matrix yields eigenvalues and eigenvectors.\n",
    "# Principal Components:\n",
    "# Selection: Principal components are chosen as eigenvectors corresponding to the largest eigenvalues.\n",
    "# Maximization: These components maximize the variance along their directions.\n",
    "# Projection:\n",
    "# Transformation: Data is projected onto the selected principal components.\n",
    "# Objective: Achieve dimensionality reduction while retaining as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dda466-7a3b-49b8-b56c-85db36e36cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "# Relationship between covariance matrices and PCA:\n",
    "# Covariance Matrix: Represents the covariances b/w pairs of features in the dataset.\n",
    "# PCA Connection:\n",
    "    # Calculation: PCA involves computing the covariance matrix of the original data.\n",
    "    # Eigenvalue Decomposition: The covariance matrix is then diagonalized to obtain eigenvalues and eigenvectors.\n",
    "# Principal Components:\n",
    "    # Selection: Principal components are derived from the eigenvectors of the covariance matrix.\n",
    "    # Variance Maximization: These components capture directions of maximum variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10531643-aa8c-42cb-bf7d-341eafe61034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# Impact of Number of Principal Components in PCA:\n",
    "    # Underfitting vs Overfitting:\n",
    "        # Fewer Components: May lead to underfitting, insufficiently capturing data variance.\n",
    "        # More Components: Risk of overfitting, capturing noise and reducing generalization.\n",
    "    # Optimal Selection:\n",
    "        # Balacing Act: Optimal number balances model complexity and variance retention.\n",
    "    # Explained Variance:\n",
    "        # Metric: Consider cumulative explained variance to guide the choice.\n",
    "    # Computational Efficiency:\n",
    "        # Consideration: More components increase computational demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77daa27-ef59-4ed0-8e06-fa031322ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.\n",
    "\n",
    "# PCA in Feature Selection:\n",
    "# Method:\n",
    "    # Transformative: PCA transforms the original features into principal components.\n",
    "    # Ranking: Features are ranked based on their contribution to variance.\n",
    "# Benefits:\n",
    "    # Reduction of Redundancy: Identifies and eliminates redundant features.\n",
    "    # Efficient Subset: Enables the selection of an efficient subset of features.\n",
    "    # Dimensionality Reduction: Diminishes the number of features while preserving information.\n",
    "    # Mitigation of overfitting: Reduces the risk of overfitting by focusing on informative features.\n",
    "    # Enhanced Model Performance: Improves model efficiency and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f3f6b0-b305-4502-8afc-cb7830551408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "\n",
    "# Common Applications of PCA:\n",
    "    # Dimensionality Reduction: Reduce data dimensionality while preserving relevant information.\n",
    "    # Noise Reduction: Filters out noise and captures essential patterns.\n",
    "    # Feature Engineering: Extracts important features and enhances model performance.\n",
    "    # Image Compression: Compresses images by retaining significant components.\n",
    "    # Face Recognition: Extracts facial features for efficient recognition.\n",
    "    # Biomedical Data Analysis: Analyzes and interprets high-dimensional biological data.\n",
    "    # Speech Recognition: Represents speech features compactly for recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ec1705-ca68-4c00-9ca9-3b38171a36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. \n",
    "\n",
    "# Relationship b/w spread and variance in PCA:\n",
    "    # Variance: It measures the spread or dispersion of data.\n",
    "    \n",
    "    # Spread in PCA:\n",
    "        #Connection: Spread in PCA refers to the variance along each principal component.\n",
    "        # Principal Components: Each principal component captures a specific direction of maximum variance.\n",
    "    # Maximizing Spread:\n",
    "        # Objective: PCA aims to find principal components that mazimize the spread or variance in the data.\n",
    "        # Efficient Representation: Principal components represent dimensions of greatest variability in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98aff082-411c-4200-a5e5-5283dc38df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8.\n",
    "\n",
    "# PCA and identification of principal components:\n",
    "    # Spread Utilization: PCA identifies principal components aligned with directions of maximum data spread or variance.\n",
    "    # Covariance Matrix: Computes the covariance matrix to quantify relationships b/w features.\n",
    "    # Eigenvalue Decomposition: Eigenvalue decomposition of the covariance matrix reveals principal components.\n",
    "    # Variance Maximization: Principal components correspond to eigenvectors that maximize variance.\n",
    "    # Principal Component Extraction: Extracts principal components representing significant directions of data variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0946d-cd5c-4cf7-9ce6-26a1088f5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9.\n",
    "\n",
    "# Handling High Variance in PCA:\n",
    "    # Identification: \n",
    "        # Recognition: PCA identifies dimensions with high variance.\n",
    "    # Emphasis on Variability:\n",
    "        # Focus: Emphasizes dimensions with significant variance in the data.\n",
    "    # Weighted Contribution:\n",
    "        # Influence: Principal components capture and prioritize high-variance dimesnions,\n",
    "    # Dimension Reduction:\n",
    "        # Effect: Reduces the impact of low-variance dimensions in the representation.\n",
    "    # Efficient D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
